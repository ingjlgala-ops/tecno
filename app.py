import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

st.set_page_config(page_title="TECNO SOPORTE VIRTUAL GPT", page_icon="ü§ñ")
st.title("ü§ñ TECNO ChatGPT ")

# üé• Preguntas con video de YouTube
VIDEO_RESPUESTAS = {
    "que es python": {
        "texto": "Aqu√≠ tienes un video para aprender qu√© es Python üêç",
        "video": "https://www.youtube.com/watch?v=rfscVS0vtbw"
    },
    "que es inteligencia artificial": {
        "texto": "Este video explica la Inteligencia Artificial ü§ñ",
        "video": "https://www.youtube.com/watch?v=2ePf9rue1Ao"
    },
    "que es machine learning": {
        "texto": "Aprende Machine Learning con este video üìä",
        "video": "https://www.youtube.com/watch?v=ukzFI9rgwfU"
    },
    "como crear un chatbot": {
        "texto": "Este video te ense√±a c√≥mo crear un chatbot üí¨",
        "video": "https://www.youtube.com/watch?v=JMUxmLyrhSk"
    }
}

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
    model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
    return tokenizer, model

tokenizer, model = load_model()

if "chat_history_ids" not in st.session_state:
    st.session_state.chat_history_ids = None

st.subheader("üìå Preguntas con video")
for pregunta in VIDEO_RESPUESTAS:
    if st.button(pregunta):
        st.markdown(f"**ü§ñ Bot:** {VIDEO_RESPUESTAS[pregunta]['texto']}")
        st.video(VIDEO_RESPUESTAS[pregunta]["video"])

user_input = st.text_input("Escribe tu mensaje:")

if st.button("Enviar"):
    if user_input:
        user_text = user_input.lower().strip()

        # üé• Si es pregunta con video
        if user_text in VIDEO_RESPUESTAS:
            st.markdown(f"**ü§ñ Bot:** {VIDEO_RESPUESTAS[user_text]['texto']}")
            st.video(VIDEO_RESPUESTAS[user_text]["video"])

        # ü§ñ Chatbot normal
        else:
            new_input_ids = tokenizer.encode(
                user_input + tokenizer.eos_token,
                return_tensors="pt"
            )

            if st.session_state.chat_history_ids is not None:
                bot_input_ids = torch.cat(
                    [st.session_state.chat_history_ids, new_input_ids], dim=-1
                )
            else:
                bot_input_ids = new_input_ids

            st.session_state.chat_history_ids = model.generate(
                bot_input_ids,
                max_length=1000,
                pad_token_id=tokenizer.eos_token_id
            )

            response = tokenizer.decode(
                st.session_state.chat_history_ids[:, bot_input_ids.shape[-1]:][0],
                skip_special_tokens=True
            )

            st.markdown(f"**ü§ñ Bot:** {response}")
